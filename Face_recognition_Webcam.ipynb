{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory: .\n",
      "\tFace_detection_Webcam.ipynb\n",
      "\topenface.nn4.small2.v1.t7\n",
      "\tFace_recognition_Webcam.ipynb\n",
      "Found directory: ./dataset\n",
      "Found directory: ./dataset/claudia\n",
      "\tclau_6.jpg\n",
      "\tclau_5.jpg\n",
      "\tclau_1.jpeg\n",
      "\tclau_4.jpg\n",
      "\tclau_2.jpg\n",
      "\tclau_3.jpg\n",
      "Found directory: ./dataset/unknown\n",
      "\tdesconocido_2.jpeg\n",
      "\tdesconocido_1.jpeg\n",
      "Found directory: ./dataset/daniel\n",
      "\tmi_foto.jpg\n",
      "\tdaniel_2.jpg\n",
      "\tdaniel_1.jpg\n",
      "\tdaniel_3.jpg\n",
      "\tdaniel_4.jpg\n",
      "Found directory: ./.ipynb_checkpoints\n",
      "\tFace_recognition_Webcam-checkpoint.ipynb\n",
      "\tFace_detection_Webcam-checkpoint.ipynb\n",
      "Found directory: ./output\n",
      "\tembeddings.pickle\n",
      "\tle.pickle\n",
      "\trecognizer.pickle\n",
      "Found directory: ./face_detection_model\n",
      "\tres10_300x300_ssd_iter_140000.caffemodel\n",
      "\tdeploy.prototxt.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the directory you want to start from\n",
    "rootDir = '.'\n",
    "for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "    print('Found directory: %s' % dirName)\n",
    "    for fname in fileList:\n",
    "        print('\\t%s' % fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.- Load face detector Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector...\n",
      "[INFO] loading face recognizer...\n"
     ]
    }
   ],
   "source": [
    "# load our serialized face detector from disk\n",
    "print(\"[INFO] loading face detector...\")\n",
    "path_prototxt = \"face_detection_model/deploy.prototxt.txt\"\n",
    "path_model = \"face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "th_confidence = 0.5\n",
    "detector = cv2.dnn.readNetFromCaffe(path_prototxt, path_model)\n",
    "# load our serialized face embedding model from disk\n",
    "print(\"[INFO] loading face recognizer...\")\n",
    "embedding_model = \"openface.nn4.small2.v1.t7\"\n",
    "embeddings_output = \"output/embeddings.pickle\"\n",
    "embedder = cv2.dnn.readNetFromTorch(embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.-Extract embeddings from face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n",
      "[INFO] processing image 1/13\n",
      "[INFO] processing image 2/13\n",
      "[INFO] processing image 3/13\n",
      "[INFO] processing image 4/13\n",
      "[INFO] processing image 5/13\n",
      "[INFO] processing image 6/13\n",
      "[INFO] processing image 7/13\n",
      "[INFO] processing image 8/13\n",
      "[INFO] processing image 9/13\n",
      "[INFO] processing image 10/13\n",
      "[INFO] processing image 11/13\n",
      "[INFO] processing image 12/13\n",
      "[INFO] processing image 13/13\n",
      "[INFO] serializing 12 encodings...\n"
     ]
    }
   ],
   "source": [
    "# grab the paths to the input images in our dataset\n",
    "print(\"[INFO] quantifying faces...\")\n",
    "path_dataset = \"dataset\"\n",
    "imagePaths = list(paths.list_images(path_dataset))\n",
    "# initialize our lists of extracted facial embeddings and\n",
    "# corresponding people names\n",
    "knownEmbeddings = []\n",
    "knownNames = []\n",
    "# initialize the total number of faces processed\n",
    "total = 0\n",
    "\n",
    "# loop over the image paths\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # extract the person name from the image path\n",
    "    print(\"[INFO] processing image {}/{}\".format(i + 1,\n",
    "        len(imagePaths)))\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    # load the image, resize it to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image\n",
    "    # dimensions\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=600)\n",
    "    (h, w) = image.shape[:2]\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "    # ensure at least one face was found\n",
    "    if len(detections) > 0:\n",
    "        # we're making the assumption that each image has only ONE\n",
    "        # face, so find the bounding box with the largest probability\n",
    "        i = np.argmax(detections[0, 0, :, 2])\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # ensure that the detection with the largest probability also\n",
    "        # means our minimum probability test (thus helping filter out\n",
    "        # weak detections)\n",
    "        if confidence > th_confidence:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            # extract the face ROI and grab the ROI dimensions\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "                \n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "            # add the name of the person + corresponding face\n",
    "            # embedding to their respective lists\n",
    "            knownNames.append(name)\n",
    "            knownEmbeddings.append(vec.flatten())\n",
    "            total += 1\n",
    "            \n",
    "# dump the facial embeddings + names to disk\n",
    "print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "f = open(embeddings_output, \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Train recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face embeddings...\n",
      "[INFO] encoding labels...\n",
      "[INFO] training model...\n"
     ]
    }
   ],
   "source": [
    "output_recognizer = \"output/recognizer.pickle\"\n",
    "output_label = \"output/le.pickle\"\n",
    "embeddings_output = \"output/embeddings.pickle\"\n",
    "\n",
    "# load the face embeddings\n",
    "print(\"[INFO] loading face embeddings...\")\n",
    "data = pickle.loads(open(embeddings_output, \"rb\").read())\n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data[\"names\"])\n",
    "\n",
    "# train the model used to accept the 128-d embeddings of the face and\n",
    "# then produce the actual face recognition\n",
    "print(\"[INFO] training model...\")\n",
    "recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "recognizer.fit(data[\"embeddings\"], labels)\n",
    "# write the actual face recognition model to disk\n",
    "f = open(output_recognizer, \"wb\")\n",
    "f.write(pickle.dumps(recognizer))\n",
    "f.close()\n",
    "# write the label encoder to disk\n",
    "f = open(output_label, \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Load Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector...\n",
      "[INFO] loading face recognizer...\n"
     ]
    }
   ],
   "source": [
    "path_prototxt = \"face_detection_model/deploy.prototxt.txt\"\n",
    "path_model = \"face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "th_confidence = 0.5\n",
    "output_recognizer = \"output/recognizer.pickle\"\n",
    "output_label = \"output/le.pickle\"\n",
    "embeddings_output = \"output/embeddings.pickle\"\n",
    "embedding_model = \"openface.nn4.small2.v1.t7\"\n",
    "\n",
    "# load our serialized face detector from disk\n",
    "print(\"[INFO] loading face detector...\")\n",
    "\n",
    "detector = cv2.dnn.readNetFromCaffe(path_prototxt, path_model)\n",
    "# load our serialized face embedding model from disk\n",
    "print(\"[INFO] loading face recognizer...\")\n",
    "embedder = cv2.dnn.readNetFromTorch(embedding_model)\n",
    "# load the actual face recognition model along with the label encoder\n",
    "recognizer = pickle.loads(open(output_recognizer, \"rb\").read())\n",
    "le = pickle.loads(open(output_label, \"rb\").read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- Recognition in Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n",
      "[INFO] elasped time: 126.51\n",
      "[INFO] approx. FPS: 17.10\n"
     ]
    }
   ],
   "source": [
    "# initialize the video stream, then allow the camera sensor to warm up\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "# start the FPS throughput estimator\n",
    "fps = FPS().start()\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream\n",
    "    frame = vs.read()\n",
    "    # resize the frame to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image\n",
    "    # dimensions\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    (h, w) = frame.shape[:2]\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections\n",
    "        if confidence > th_confidence:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            # extract the face ROI\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "        # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "            # perform classification to recognize the face\n",
    "            preds = recognizer.predict_proba(vec)[0]\n",
    "            j = np.argmax(preds)\n",
    "            proba = preds[j]\n",
    "            name = le.classes_[j]\n",
    "            # draw the bounding box of the face along with the\n",
    "            # associated probability\n",
    "            text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "            cv2.putText(frame, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "                \n",
    "            # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
